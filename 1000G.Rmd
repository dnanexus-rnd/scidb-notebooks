---
title: "1000 Genomes Exercises in SciDB"
output:
  html_document:
    pandoc_args: ["+RTS", "-K16g", "-RTS"]
---

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
# http://stackoverflow.com/questions/1716012/stopwatch-function-in-r/1716344#1716344
tic <- function(gcFirst = TRUE, type=c("elapsed", "user.self", "sys.self"))
{
   type <- match.arg(type)
   assign(".type", type, envir=baseenv())
   if(gcFirst) gc(FALSE)
   tic <- proc.time()[type]         
   assign(".tic", tic, envir=baseenv())
   invisible(tic)
}

toc <- function()
{
   type <- get(".type", envir=baseenv())
   toc <- proc.time()[type]
   tic <- get(".tic", envir=baseenv())
   print(toc - tic)
   invisible(toc)
}
```

This workbook demonstrates using SciDB's R interface to query and analyze 1000 Genomes phase 3 genotype data. First, we connect to SciDB and obtain handles to the data arrays. For development/testing, we'll just look at chromosomes 21 and 22 across the 2,504 individuals.

```{r, warning=FALSE, error=FALSE, message=FALSE}
require(ggplot2)
require(scidb)
scidbconnect()
SAMPLE <- scidb("KG_SAMPLE")
CHROMOSOME <- scidb("KG_CHROMOSOME")
VARIANT <- merge(scidb("KG_VARIANT"), subset(CHROMOSOME, "chrom='21' or chrom='22'"))
VARIANT <- scidbeval(VARIANT)
GENOTYPE <- project(merge(scidb("KG_GENOTYPE_PARSED"),project(VARIANT,'chrom')),
                    c("allele1","allele2","phased"))
```

The `VARIANT` array for chomosomes 21 and 22 has been stored in SciDB's memory (but not the memory of our local R process) by the `scidbeval()` expression above. In contrast, the `GENOTYPE` array is merely a lazy expression, denoting but not materializing the subset of all the genotypes on chromosomes 21 and 22. Because `GENOTYPE` is our biggest array by far, we avoid copying any significant portion of it.

Let's take a look at the schema.

```{r, warning=FALSE}
head(VARIANT)
str(VARIANT)
count(VARIANT)

GENOTYPE[12345:12350,0,][]
str(GENOTYPE)
count(GENOTYPE)
```

## Transition/transversion ratio

The transition/transversion ratio (Ti/Tv) is a common quality metric for variant call sets. Let's compute Ti/Tv of all the variants with respect to the reference genome. This first calculation is on the variants only, not the individuals' genotypes, and thus involves only a modest amount of data.

```{r}
# count biallelic SNPs
SNP <- subset(VARIANT, "(ref='A' or ref='G' or ref='C' or ref='T') and
                        (alt='A' or alt='G' or alt='C' or alt='T')")
snps <- count(SNP)
snps

# annotate each SNP as to whether it's a transition (or else transversion)
transitions_filter_str <- "(ref='A' and alt='G') or (ref='G' and alt='A') or
                           (ref='C' and alt='T') or (ref='T' and alt='C')"
SNP <- bind(SNP,"is_transition",paste("bool(iif(", transitions_filter_str, ",TRUE,FALSE))"))
SNP <- scidbeval(SNP)

# count transitions
ti <- count(SNP$is_transition %==% TRUE)
ti

# count transversions
tv <- count(SNP$is_transition %==% FALSE)
tv

# report Ti/Tv
stopifnot(ti+tv == snps)
ti/tv
```

How about Ti/Tv of one individual's genotypes, with respect to the reference?

```{r}
GENOTYPE_HG03209 <- merge(GENOTYPE, SAMPLE$sample_name %==% "HG03209")
GENOTYPE_HG03209_TI <- merge(GENOTYPE_HG03209, SNP$is_transition %==% TRUE, "variant_id")
ti_HG03209 <- aggregate(bind(GENOTYPE_HG03209_TI, "alt_copies", "allele1+allele2"),
                        FUN="sum(alt_copies)")[]
ti_HG03209

GENOTYPE_HG03209_TV <- merge(GENOTYPE_HG03209, SNP$is_transition %==% FALSE, "variant_id")
tv_HG03209 <- aggregate(bind(GENOTYPE_HG03209_TV, "alt_copies", "allele1+allele2"),
                        FUN="sum(alt_copies)")[]
tv_HG03209

ti_HG03209/tv_HG03209
```

And finally, let's look at the distribution of Ti/Tv across the individuals in the population.

```{r}
GENOTYPE_TI <- merge(GENOTYPE, SNP$is_transition %==% TRUE, "variant_id")
TI_BY_SAMPLE <- aggregate(bind(GENOTYPE_TI, "alt_copies", "uint8(allele1+allele2)"),
                          "sample_id","sum(alt_copies)")

GENOTYPE_TV <- merge(GENOTYPE, SNP$is_transition %==% FALSE, "variant_id")
TV_BY_SAMPLE <- aggregate(bind(GENOTYPE_TV, "alt_copies", "uint8(allele1+allele2)"),
                          "sample_id","sum(alt_copies)")

TITV_BY_SAMPLE <- project(bind(TI_BY_SAMPLE,'ti_d','double(alt_copies_sum)'),'ti_d')/TV_BY_SAMPLE
TITV_BY_SAMPLE <- scidbeval(TITV_BY_SAMPLE)

invisible(hist(TITV_BY_SAMPLE[1:count(SAMPLE)-1], xlab="Ti/Tv", ylab="# individuals", main=""))
```

All the data traversal is performed by SciDB in parallel; then the histogram buckets and counts are imported into R memory for plotting. Note how lazily-evaluated subexpressions can be stored, composed, and reused as R variables. This is often a lot nicer than formulating SQL.

On the other hand, small arrays can easily be moved back and forth between SciDB and R, with one pitfall: SciDB arrays are zero-based, while R uses one-based indexing. But as genome scientists, we're right at home dealing with this ;-)

## Principal component analysis

Now let's find principal components of the genotype data and project the individual genomes onto them, revealing the underlying population structure. Begin by selecting common SNPs, because rare variants inherently don't contribute much to the overall variance.

```{r}
SNP_COMMON <- subset(merge(SNP,scidb("KG_VARIANT_MULT_VAL")),
                     sprintf("af>= %f and af <=%f ", 0.1, 0.9))
SNP_COMMON <- redimension(SNP_COMMON, sprintf("<signature:string, af:double NULL> [variant_id=0:%i,10000,0]",
                                              nrow(SNP_COMMON)-1))
SNP_COMMON <- scidbeval(SNP_COMMON)
count(SNP_COMMON)
```

The redimension operation just removes some unneeded attributes and dimensions.

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
# downsampling for test/dev:
# SNP_COMMON <- scidbeval(bernoulli(SNP_COMMON, 0.1, 42))
# count(SNP_COMMON)
```

Now construct a matrix `G` where `G[i,j]` is the number of copies of the alt allele found in sample `i`, common SNP `j`. The samples are observations, and the SNPs are variables. Reference: doi:10.1371/journal.pgen.0020190

```{r}
# construct a dense matrix of alt allele counts from the selected sites
Gdensifier <- dimension_rename(unpack(SNP_COMMON),"i","dense_variant_id") 
Gdensifier <- redimension(Gdensifier, sprintf("<dense_variant_id:int64> %s", 
                                              scidb:::build_dim_schema(VARIANT))) 

G <- bind(GENOTYPE, "alt_count", "double(allele1+allele2)") 
G <- redimension(merge(G, Gdensifier), 
                 sprintf("<alt_count:double NULL> [sample_id = 0:%i,1000,0, dense_variant_id=0:%i,1000,0]", 
                         count(SAMPLE)-1, count(SNP_COMMON)-1)) 
G <- scidbeval(G, temp=TRUE) 
dim(G) 
head(G) 
```

Normalize each variable and compute the sample covariance matrix ("economy-sized", since we have many more variables than observations):

```{r}
# Normalization factor for the variance of each site - proportional to sqrt(p*(1-p))
Gnormalizer <- replaceNA(bind(SNP_COMMON, "u", "pow((1-af)*af,0.5)")) 
Gnormalizer <- redimension(merge(Gnormalizer, Gdensifier), 
                           sprintf("<u:double> [dense_variant_id=0:%i,1000,0]", 
                                   count(SNP_COMMON)-1))

# subtract means and apply variance normalization
G0 <- sweep(G, 2, apply(G, 2, mean))
G0 <- project(bind(merge(G0,Gnormalizer), "v", "alt_count/u"), "v")
G0 <- scidbeval(G0, temp=TRUE)

# compute sample covariance
SCV <- scidbeval(tcrossprod(G0)/(ncol(G0)-1), temp=TRUE)
invisible(image(SCV))
```

Perform SVD on the covariance matrix, and plot the projection of the observations (individuals) onto the first few principal components:

```{r}
SCVsvd <- scidbeval(svd(SCV), temp=TRUE)
H <- SCVsvd$u[,0:9][] %*% diag(SCVsvd$d[0:9][])
qplot(H[,1], H[,2], xlab="PC1", ylab="PC2")
```

For the avoidance of doubt: all the matrix calculations were parallelized in SciDB, not computed by the local R process. We didn't have to dump a lot of data into any separate system for numerical analysis.

The PCA's interpretation becomes clear if we label the points by the ethnicity of each individual. This also provides an elementary example of joining "phenotypic data":

```{r, echo=FALSE}
pd <- data.frame(PC1=H[,1], PC2=H[,2], PC3=H[,3],
                 population=scidb("KG_SAMPLE_POPL_CODE")$Population[])

popnames <- list(
  ACB='Caribbean',
  ASW='African-Amer',
  BEB='South Asian',
  CEU='European',
  CHB='East Asian',
  CDX='East Asian',
  CHS='East Asian',
  CLM='Central Amer',
  ESN='African',
  FIN='European',
  GBR='European',
  GIH='South Asian',
  GWD='African',
  IBS='European',
  ITU='South Asian',
  JPT='East Asian',
  KHV='East Asian',
  LWK='African',
  MSL='African',
  MXL='Central Amer',
  PEL='South Amer',
  PJL='South Asian',
  PUR='Caribbean',
  STU='South Asian',
  TSI='European',
  YRI='African'
)
for (i in 1:length(popnames)) {
  levels(pd$population)[levels(pd$population) == names(popnames)[i]] <- popnames[[names(popnames)[i]]]
}

cbbPalette <- c("#E69F00", "#CC79A7", "#D55E00", "#56B4E9", "#009E73", "#999999", "#0072B2", "#FF0000")

ggplot(pd, aes(PC1,PC2,shape=population,color=population)) + geom_point() + scale_shape_manual(values=(1:nrow(pd))+1) + scale_color_manual(values=cbbPalette) + theme(panel.background=element_blank(), legend.text=element_text(size=10))

ggplot(pd, aes(PC2,PC3,shape=population,color=population)) + geom_point() + scale_shape_manual(values=(1:nrow(pd))+1) + scale_color_manual(values=cbbPalette) + theme(panel.background=element_blank(), legend.text=element_text(size=10))
```

In addition to generating cool visualizations, PCA plays a crucial role in statistically correcting for population stratification in large-scale genetic association studies.

## Linkage disequilibrium

Linkage disequilibrum (LD) describes the correlation among the alleles observed at nearby sites. It reflects the inheritance of haplotypes on short timescales compared to the rate of genetic recombination. Let's look at LD among common SNPs in a region of several hundred kilobases.

```{r}
LDchrom <- "21"
LDlo <- 35409243
LDhi <- 36065894
LD_SNP <- subset(SNP, sprintf("chrom = '%s' and pos >= %i and pos < %i",
                              LDchrom, LDlo, LDhi))
LD_SNP <- subset(merge(LD_SNP, scidb("KG_VARIANT_MULT_VAL")),
                 sprintf("af>= %f and af <=%f ", 0.1, 0.9))
LD_SNP <- redimension(LD_SNP, sprintf("<signature:string> [variant_id=0:%i,10000,0]",
                                      nrow(LD_SNP)-1))
LD_SNP <- scidbeval(LD_SNP)
count(LD_SNP)
```

Formulate L where L[i,j] is the alt allele count in sample i at site j.

```{r}
L <- bind(merge(GENOTYPE, LD_SNP, "variant_id"), "alt_count", "double(allele1+allele2)")
L <- index_lookup(L, unique(LD_SNP$signature), "signature", "dense_variant_id")
L <- redimension(L, sprintf("<alt_count:double NULL> [sample_id=0:%i,313,0, dense_variant_id=0:%i,100,0]",
                            count(SAMPLE)-1, count(LD_SNP)-1))
L <- scidbeval(L)
```

Compute the sample correlation matrix for the sites, providing estimates of the linkage disequilibrium (r) for each pair of SNPs. Reference: doi:10.1534/genetics.108.093153

```{r}
# reference: https://github.com/Paradigm4/SciDBR/wiki/Correlation-matrix-example
L0 <- sweep(L, 2, apply(L, 2, mean))
Lcov <- crossprod(L0)/(nrow(L0) - 1)
Ls  <- diag(Lcov)^(-1/2)
Lr <- Ls * Lcov * Ls
Lr2 <- scidbeval(project(bind(Lr, "r2", "pow(v,2)"), "r2"))
invisible(image(Lr2,main=sprintf("LD among common SNPs in %s:%d-%d",
                                 LDchrom, LDlo, LDhi)))
```

We see several distinct haplotype blocks in this region. Unusually long haplotype blocks - which these are not necessarily - can indicate interesting population history, such as selective sweeps, bottlenecks, or founder effects.

## Hardy-Weinberg equilibrium

Hardy-Weinberg equilibrium (HWE) describes the diploid genotype frequencies expected as a function of the allele frequencies at a site, under various idealistic assumptions. Let's count the genotypes observed across the population at each SNP:

```{r, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
#tic()
# unbelievably slow:
#GT_CTS <- aggregate(merge(GENOTYPE, SNP, "variant_id"), by=list("variant_id", "allele1", "allele2"), FUN="count(*)")
#GT_CTS <- scidbeval(GT_CTS)
#toc()
```

```{r}
GENOTYPE_SNP <- project(merge(GENOTYPE, SNP, "variant_id"), c("allele1","allele2"))
GT_HOM0_BY_SNP <- aggregate(subset(GENOTYPE_SNP, "allele1=0 and allele2=0"),
                            by="variant_id", FUN="count(allele1)", eval=TRUE)
GT_HOM1_BY_SNP <- aggregate(subset(GENOTYPE_SNP, "allele1=1 and allele2=1"),
                            by="variant_id", FUN="count(allele1)", eval=TRUE)
GT_HET_BY_SNP <- aggregate(subset(GENOTYPE_SNP, "(allele1=1 and allele2=0) or
                                                 (allele1=0 and allele2=1)"),
                           by="variant_id", FUN="count(allele1)", eval=TRUE)

# merge (cbind) these three vectors. Use all=TRUE since they're sparse; otherwise,
# we'd lose rows where any of the three entries is empty.
GT_BY_SNP <- merge(attribute_rename(GT_HOM0_BY_SNP, "allele1_count", "AA"),
             merge(attribute_rename(GT_HET_BY_SNP, "allele1_count", "Aa"),
                   attribute_rename(GT_HOM1_BY_SNP, "allele1_count", "aa"),
                   all=TRUE), all=TRUE)
stopifnot(count(GT_BY_SNP) == count(SNP))
GT_BY_SNP <- scidbeval(replaceNA(GT_BY_SNP))
GT_BY_SNP[0:9,][]
```

Add the allele frequencies:

```{r}
HW_BY_SNP <- bind(GT_BY_SNP, "N", "AA + Aa + aa")
HW_BY_SNP <- bind(HW_BY_SNP, "p", "double(2*AA + Aa)/(2*N)")
HW_BY_SNP <- bind(HW_BY_SNP, "q", "double(2*aa + Aa)/(2*N)")
HW_BY_SNP <- scidbeval(HW_BY_SNP)
head(HW_BY_SNP)
```

Compute the expected genotype counts under HWE, and the chi^2 statistics for deviation from equilibrium.

```{r}
HW_BY_SNP <- bind(HW_BY_SNP, "E_AA", "N*p*p")
HW_BY_SNP <- bind(HW_BY_SNP, "E_Aa", "2*N*p*q")
HW_BY_SNP <- bind(HW_BY_SNP, "E_aa", "N*q*q")
HW_BY_SNP <- bind(HW_BY_SNP, "chi_2", "pow(AA-E_AA,2)/E_AA +
                                       pow(Aa-E_Aa,2)/E_Aa +
                                       pow(aa-E_aa,2)/E_aa")
HW_BY_SNP <- scidbeval(HW_BY_SNP)
round(head(HW_BY_SNP),4)

# plot a sample of Aa vs. E_Aa
HW_BY_SNP_SAMPLE <- unpack(bernoulli(HW_BY_SNP, 0.005))
HW_BY_SNP_SAMPLE <- HW_BY_SNP_SAMPLE[1:count(HW_BY_SNP_SAMPLE)-1,] # trims dimension
qplot(as(HW_BY_SNP_SAMPLE$E_Aa[],"vector"), as(HW_BY_SNP_SAMPLE$Aa[],"vector"),
      xlab="E_Aa", ylab="Aa")

# proportion of sites with chi_2 > 3.841; random expectation would be 0.05.
count(HW_BY_SNP$chi_2 %>% 3.841)/count(HW_BY_SNP$chi_2)
```

Have a look at the most out-of-equilibrium common SNPs:

```{r}
HWE_OUTLIERS <- sort(merge(project(SNP_COMMON,"signature"), HW_BY_SNP),
                     decreasing=TRUE, attributes="chi_2")
head(HWE_OUTLIERS[,c("signature","AA","Aa","aa","chi_2")])
```

Close examination of these sites would probably reveal regions prone to ambiguous NGS read mapping, or some other technical artifacts. Occasionally, after careful follow-up analysis, we might find sites out of equilibrium for interesting reasons, such as recent selective sweeps or heterozygote advantage.
